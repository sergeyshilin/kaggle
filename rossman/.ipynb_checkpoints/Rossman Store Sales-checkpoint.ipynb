{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import cross_validation\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dateparse = lambda x: datetime.strptime(x, '%Y-%m-%d')\n",
    "train = pd.read_csv('data/train.csv', low_memory=False, parse_dates = ['Date'])\n",
    "test = pd.read_csv('data/test.csv', low_memory=False, parse_dates = ['Date'])\n",
    "stores = pd.read_csv('data/store.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store            0\n",
      "DayOfWeek        0\n",
      "Date             0\n",
      "Sales            0\n",
      "Customers        0\n",
      "Open             0\n",
      "Promo            0\n",
      "StateHoliday     0\n",
      "SchoolHoliday    0\n",
      "dtype: int64\n",
      "Id                0\n",
      "Store             0\n",
      "DayOfWeek         0\n",
      "Date              0\n",
      "Open             11\n",
      "Promo             0\n",
      "StateHoliday      0\n",
      "SchoolHoliday     0\n",
      "dtype: int64\n",
      "Store                          0\n",
      "StoreType                      0\n",
      "Assortment                     0\n",
      "CompetitionDistance            3\n",
      "CompetitionOpenSinceMonth    354\n",
      "CompetitionOpenSinceYear     354\n",
      "Promo2                         0\n",
      "Promo2SinceWeek              544\n",
      "Promo2SinceYear              544\n",
      "PromoInterval                544\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print train.isnull().sum(axis=0)\n",
    "print test.isnull().sum(axis=0)\n",
    "print stores.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.fillna(1, inplace=True)\n",
    "train = train[train[\"Open\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge datasets with Stores data\n",
    "train = pd.merge(train, stores, on='Store')\n",
    "test = pd.merge(test, stores, on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_features(data):\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.loc[data.Open.isnull(), 'Open'] = 1\n",
    "\n",
    "    dates = pd.DatetimeIndex(data.Date)\n",
    "    data['Day'] = data.Date.apply(lambda x: x.day)\n",
    "    data['Month'] = data.Date.apply(lambda x: x.month)\n",
    "    data['Year'] = data.Date.apply(lambda x: x.year)\n",
    "    data['WeekOfYear'] = data.Date.apply(lambda x: x.weekofyear)\n",
    "    \n",
    "    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + (data.Month - data.CompetitionOpenSinceMonth)\n",
    "    data['CompetitionOpen'] = data.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + (data.WeekOfYear - data.Promo2SinceWeek) / float(4)\n",
    "    data['PromoOpen'] = data.CompetitionOpen.apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    data['p_1'] = data.PromoInterval.apply(lambda x: x[:3] if type(x) == str else 0)\n",
    "    data['p_2'] = data.PromoInterval.apply(lambda x: x[4:7] if type(x) == str else 0)\n",
    "    data['p_3'] = data.PromoInterval.apply(lambda x: x[8:11] if type(x) == str else 0)\n",
    "    data['p_4'] = data.PromoInterval.apply(lambda x: x[12:15] if type(x) == str else 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_features(train)\n",
    "build_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Store                                 int64\n",
       "DayOfWeek                             int64\n",
       "Date                         datetime64[ns]\n",
       "Sales                                 int64\n",
       "Customers                             int64\n",
       "Open                                  int64\n",
       "Promo                                 int64\n",
       "StateHoliday                         object\n",
       "SchoolHoliday                         int64\n",
       "StoreType                            object\n",
       "Assortment                           object\n",
       "CompetitionDistance                 float64\n",
       "CompetitionOpenSinceMonth           float64\n",
       "CompetitionOpenSinceYear            float64\n",
       "Promo2                                int64\n",
       "Promo2SinceWeek                     float64\n",
       "Promo2SinceYear                     float64\n",
       "PromoInterval                        object\n",
       "Day                                   int64\n",
       "Month                                 int64\n",
       "Year                                  int64\n",
       "WeekOfYear                            int64\n",
       "CompetitionOpen                     float64\n",
       "PromoOpen                           float64\n",
       "p_1                                  object\n",
       "p_2                                  object\n",
       "p_3                                  object\n",
       "p_4                                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace labels with floats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_enc = LabelEncoder()\n",
    "\n",
    "for c in ['StateHoliday', 'StoreType', 'Assortment', 'p_1', 'p_2', 'p_3', 'p_4']:\n",
    "    train[c] = lbl_enc.fit_transform(train[c])\n",
    "    test[c] = lbl_enc.transform(test[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: \n",
      "['Store', 'DayOfWeek', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance', 'Promo2', 'Day', 'Month', 'Year', 'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'p_1', 'p_2', 'p_3', 'p_4']\n"
     ]
    }
   ],
   "source": [
    "# Choose columns\n",
    "features = list(train.columns)\n",
    "[features.remove(c) for c in ['Date', 'Sales', 'Customers', 'PromoInterval', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2SinceYear', 'Promo2SinceWeek']]\n",
    "print \"Features: \"\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Store', u'DayOfWeek', u'Date', u'Sales', u'Customers', u'Open',\n",
       "       u'Promo', u'StateHoliday', u'SchoolHoliday', u'StoreType',\n",
       "       u'Assortment', u'CompetitionDistance', u'CompetitionOpenSinceMonth',\n",
       "       u'CompetitionOpenSinceYear', u'Promo2', u'Promo2SinceWeek',\n",
       "       u'Promo2SinceYear', u'PromoInterval', u'Day', u'Month', u'Year',\n",
       "       u'WeekOfYear', u'CompetitionOpen', u'PromoOpen', u'p_1', u'p_2', u'p_3',\n",
       "       u'p_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost and predict sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set params\n",
    "\n",
    "params = {\"objective\": \"reg:linear\",\n",
    "          \"booster\": \"gbtree\",\n",
    "          \"eta\": 0.02,\n",
    "          \"max_depth\": 8,\n",
    "          \"subsample\": 0.9,\n",
    "          \"colsample_bytree\": 0.7,\n",
    "          \"silent\": 1\n",
    "          }\n",
    "num_trees = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thanks to Chenglong Chen for providing this in the forum\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    # y = y.values\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check XGB score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until train error hasn't decreased in 50 rounds.\n",
      "[0]\teval-rmspe:0.996826\ttrain-rmspe:0.996807\n",
      "[1]\teval-rmspe:0.981424\ttrain-rmspe:0.981453\n",
      "[2]\teval-rmspe:0.937616\ttrain-rmspe:0.937792\n",
      "[3]\teval-rmspe:0.855453\ttrain-rmspe:0.856028\n",
      "[4]\teval-rmspe:0.741917\ttrain-rmspe:0.743132\n",
      "[5]\teval-rmspe:0.616855\ttrain-rmspe:0.619361\n",
      "[6]\teval-rmspe:0.502317\ttrain-rmspe:0.506900\n",
      "[7]\teval-rmspe:0.411179\ttrain-rmspe:0.418916\n",
      "[8]\teval-rmspe:0.351831\ttrain-rmspe:0.363371\n",
      "[9]\teval-rmspe:0.317469\ttrain-rmspe:0.330478\n",
      "[10]\teval-rmspe:0.305489\ttrain-rmspe:0.321226\n",
      "[11]\teval-rmspe:0.304282\ttrain-rmspe:0.321929\n",
      "[12]\teval-rmspe:0.302540\ttrain-rmspe:0.322267\n",
      "[13]\teval-rmspe:0.301618\ttrain-rmspe:0.321761\n",
      "[14]\teval-rmspe:0.296293\ttrain-rmspe:0.319464\n",
      "[15]\teval-rmspe:0.295883\ttrain-rmspe:0.319985\n",
      "[16]\teval-rmspe:0.294645\ttrain-rmspe:0.319298\n",
      "[17]\teval-rmspe:0.294221\ttrain-rmspe:0.319215\n",
      "[18]\teval-rmspe:0.294129\ttrain-rmspe:0.318752\n",
      "[19]\teval-rmspe:0.292351\ttrain-rmspe:0.316485\n",
      "[20]\teval-rmspe:0.280354\ttrain-rmspe:0.307090\n",
      "[21]\teval-rmspe:0.279784\ttrain-rmspe:0.305703\n",
      "[22]\teval-rmspe:0.269705\ttrain-rmspe:0.298708\n",
      "[23]\teval-rmspe:0.266947\ttrain-rmspe:0.295239\n",
      "[24]\teval-rmspe:0.264697\ttrain-rmspe:0.293204\n",
      "[25]\teval-rmspe:0.262654\ttrain-rmspe:0.290720\n",
      "[26]\teval-rmspe:0.261414\ttrain-rmspe:0.289451\n",
      "[27]\teval-rmspe:0.260209\ttrain-rmspe:0.287973\n",
      "[28]\teval-rmspe:0.251812\ttrain-rmspe:0.281369\n",
      "[29]\teval-rmspe:0.242209\ttrain-rmspe:0.272468\n",
      "[30]\teval-rmspe:0.237931\ttrain-rmspe:0.268593\n",
      "[31]\teval-rmspe:0.230698\ttrain-rmspe:0.262104\n",
      "[32]\teval-rmspe:0.229770\ttrain-rmspe:0.261249\n",
      "[33]\teval-rmspe:0.227765\ttrain-rmspe:0.259151\n",
      "[34]\teval-rmspe:0.224225\ttrain-rmspe:0.256207\n",
      "[35]\teval-rmspe:0.222639\ttrain-rmspe:0.255093\n",
      "[36]\teval-rmspe:0.219145\ttrain-rmspe:0.251693\n",
      "[37]\teval-rmspe:0.217337\ttrain-rmspe:0.249461\n",
      "[38]\teval-rmspe:0.214632\ttrain-rmspe:0.247483\n",
      "[39]\teval-rmspe:0.213327\ttrain-rmspe:0.246274\n",
      "[40]\teval-rmspe:0.209328\ttrain-rmspe:0.242950\n",
      "[41]\teval-rmspe:0.207958\ttrain-rmspe:0.241603\n",
      "[42]\teval-rmspe:0.206084\ttrain-rmspe:0.239897\n",
      "[43]\teval-rmspe:0.204119\ttrain-rmspe:0.238237\n",
      "[44]\teval-rmspe:0.201032\ttrain-rmspe:0.233805\n",
      "[45]\teval-rmspe:0.198912\ttrain-rmspe:0.231735\n",
      "[46]\teval-rmspe:0.196127\ttrain-rmspe:0.230041\n",
      "[47]\teval-rmspe:0.193790\ttrain-rmspe:0.227414\n",
      "[48]\teval-rmspe:0.192393\ttrain-rmspe:0.226105\n",
      "[49]\teval-rmspe:0.190550\ttrain-rmspe:0.224801\n",
      "[50]\teval-rmspe:0.189766\ttrain-rmspe:0.221526\n",
      "[51]\teval-rmspe:0.184956\ttrain-rmspe:0.218175\n",
      "[52]\teval-rmspe:0.184028\ttrain-rmspe:0.217316\n",
      "[53]\teval-rmspe:0.183112\ttrain-rmspe:0.216589\n",
      "[54]\teval-rmspe:0.182314\ttrain-rmspe:0.215508\n",
      "[55]\teval-rmspe:0.180991\ttrain-rmspe:0.214369\n",
      "[56]\teval-rmspe:0.180371\ttrain-rmspe:0.213717\n",
      "[57]\teval-rmspe:0.179193\ttrain-rmspe:0.215200\n",
      "[58]\teval-rmspe:0.177342\ttrain-rmspe:0.212916\n",
      "[59]\teval-rmspe:0.176787\ttrain-rmspe:0.212513\n",
      "[60]\teval-rmspe:0.174753\ttrain-rmspe:0.210866\n",
      "[61]\teval-rmspe:0.173296\ttrain-rmspe:0.209476\n",
      "[62]\teval-rmspe:0.171278\ttrain-rmspe:0.207547\n",
      "[63]\teval-rmspe:0.170250\ttrain-rmspe:0.206516\n",
      "[64]\teval-rmspe:0.169203\ttrain-rmspe:0.205871\n",
      "[65]\teval-rmspe:0.168655\ttrain-rmspe:0.205081\n",
      "[66]\teval-rmspe:0.167182\ttrain-rmspe:0.203744\n",
      "[67]\teval-rmspe:0.166716\ttrain-rmspe:0.203368\n",
      "[68]\teval-rmspe:0.164179\ttrain-rmspe:0.202687\n",
      "[69]\teval-rmspe:0.163322\ttrain-rmspe:0.201794\n",
      "[70]\teval-rmspe:0.162639\ttrain-rmspe:0.201305\n",
      "[71]\teval-rmspe:0.161408\ttrain-rmspe:0.200489\n",
      "[72]\teval-rmspe:0.160116\ttrain-rmspe:0.199667\n",
      "[73]\teval-rmspe:0.157690\ttrain-rmspe:0.197825\n",
      "[74]\teval-rmspe:0.156366\ttrain-rmspe:0.196995\n",
      "[75]\teval-rmspe:0.155733\ttrain-rmspe:0.196334\n",
      "[76]\teval-rmspe:0.155634\ttrain-rmspe:0.196376\n",
      "[77]\teval-rmspe:0.155206\ttrain-rmspe:0.196205\n",
      "[78]\teval-rmspe:0.154409\ttrain-rmspe:0.195500\n",
      "[79]\teval-rmspe:0.154264\ttrain-rmspe:0.195320\n",
      "[80]\teval-rmspe:0.153737\ttrain-rmspe:0.195188\n",
      "[81]\teval-rmspe:0.153120\ttrain-rmspe:0.194704\n",
      "[82]\teval-rmspe:0.151463\ttrain-rmspe:0.193408\n",
      "[83]\teval-rmspe:0.150584\ttrain-rmspe:0.193345\n",
      "[84]\teval-rmspe:0.149978\ttrain-rmspe:0.193248\n",
      "[85]\teval-rmspe:0.149658\ttrain-rmspe:0.192945\n",
      "[86]\teval-rmspe:0.149351\ttrain-rmspe:0.192551\n",
      "[87]\teval-rmspe:0.148218\ttrain-rmspe:0.191669\n",
      "[88]\teval-rmspe:0.147163\ttrain-rmspe:0.191719\n",
      "[89]\teval-rmspe:0.146732\ttrain-rmspe:0.191316\n",
      "[90]\teval-rmspe:0.145576\ttrain-rmspe:0.190383\n",
      "[91]\teval-rmspe:0.145124\ttrain-rmspe:0.190032\n",
      "[92]\teval-rmspe:0.143600\ttrain-rmspe:0.188967\n",
      "[93]\teval-rmspe:0.143073\ttrain-rmspe:0.188569\n",
      "[94]\teval-rmspe:0.142629\ttrain-rmspe:0.187930\n",
      "[95]\teval-rmspe:0.142467\ttrain-rmspe:0.187721\n",
      "[96]\teval-rmspe:0.142045\ttrain-rmspe:0.187375\n",
      "[97]\teval-rmspe:0.141720\ttrain-rmspe:0.187092\n",
      "[98]\teval-rmspe:0.140799\ttrain-rmspe:0.186281\n",
      "[99]\teval-rmspe:0.140495\ttrain-rmspe:0.185969\n",
      "[100]\teval-rmspe:0.140345\ttrain-rmspe:0.185782\n",
      "[101]\teval-rmspe:0.139963\ttrain-rmspe:0.185394\n",
      "[102]\teval-rmspe:0.139345\ttrain-rmspe:0.184962\n",
      "[103]\teval-rmspe:0.139129\ttrain-rmspe:0.184979\n",
      "[104]\teval-rmspe:0.138789\ttrain-rmspe:0.184691\n",
      "[105]\teval-rmspe:0.138287\ttrain-rmspe:0.183977\n",
      "[106]\teval-rmspe:0.137895\ttrain-rmspe:0.183435\n",
      "[107]\teval-rmspe:0.137692\ttrain-rmspe:0.183154\n",
      "[108]\teval-rmspe:0.137502\ttrain-rmspe:0.182996\n",
      "[109]\teval-rmspe:0.137406\ttrain-rmspe:0.182925\n",
      "[110]\teval-rmspe:0.137053\ttrain-rmspe:0.182460\n",
      "[111]\teval-rmspe:0.136739\ttrain-rmspe:0.182057\n",
      "[112]\teval-rmspe:0.136500\ttrain-rmspe:0.181640\n",
      "[113]\teval-rmspe:0.136216\ttrain-rmspe:0.181413\n",
      "[114]\teval-rmspe:0.136083\ttrain-rmspe:0.181273\n",
      "[115]\teval-rmspe:0.136041\ttrain-rmspe:0.181144\n",
      "[116]\teval-rmspe:0.135866\ttrain-rmspe:0.181007\n",
      "[117]\teval-rmspe:0.135495\ttrain-rmspe:0.180584\n",
      "[118]\teval-rmspe:0.135316\ttrain-rmspe:0.180341\n",
      "[119]\teval-rmspe:0.134881\ttrain-rmspe:0.179869\n",
      "[120]\teval-rmspe:0.134667\ttrain-rmspe:0.179646\n",
      "[121]\teval-rmspe:0.134329\ttrain-rmspe:0.179412\n",
      "[122]\teval-rmspe:0.133793\ttrain-rmspe:0.179162\n",
      "[123]\teval-rmspe:0.133498\ttrain-rmspe:0.178973\n",
      "[124]\teval-rmspe:0.133282\ttrain-rmspe:0.178805\n",
      "[125]\teval-rmspe:0.133121\ttrain-rmspe:0.178725\n",
      "[126]\teval-rmspe:0.132764\ttrain-rmspe:0.178297\n",
      "[127]\teval-rmspe:0.132428\ttrain-rmspe:0.178208\n",
      "[128]\teval-rmspe:0.132183\ttrain-rmspe:0.177942\n",
      "[129]\teval-rmspe:0.131969\ttrain-rmspe:0.177818\n",
      "[130]\teval-rmspe:0.131733\ttrain-rmspe:0.177628\n",
      "[131]\teval-rmspe:0.131187\ttrain-rmspe:0.177206\n",
      "[132]\teval-rmspe:0.131034\ttrain-rmspe:0.178297\n",
      "[133]\teval-rmspe:0.130924\ttrain-rmspe:0.178967\n",
      "[134]\teval-rmspe:0.130865\ttrain-rmspe:0.176397\n",
      "[135]\teval-rmspe:0.130659\ttrain-rmspe:0.176171\n",
      "[136]\teval-rmspe:0.130532\ttrain-rmspe:0.175963\n",
      "[137]\teval-rmspe:0.130440\ttrain-rmspe:0.175842\n",
      "[138]\teval-rmspe:0.130186\ttrain-rmspe:0.175670\n",
      "[139]\teval-rmspe:0.129908\ttrain-rmspe:0.175414\n",
      "[140]\teval-rmspe:0.129783\ttrain-rmspe:0.175303\n",
      "[141]\teval-rmspe:0.129497\ttrain-rmspe:0.175054\n",
      "[142]\teval-rmspe:0.129306\ttrain-rmspe:0.174967\n",
      "[143]\teval-rmspe:0.129097\ttrain-rmspe:0.174810\n",
      "[144]\teval-rmspe:0.129033\ttrain-rmspe:0.174742\n",
      "[145]\teval-rmspe:0.128803\ttrain-rmspe:0.174527\n",
      "[146]\teval-rmspe:0.128669\ttrain-rmspe:0.174365\n",
      "[147]\teval-rmspe:0.128532\ttrain-rmspe:0.174232\n",
      "[148]\teval-rmspe:0.128319\ttrain-rmspe:0.174085\n",
      "[149]\teval-rmspe:0.128131\ttrain-rmspe:0.173930\n",
      "[150]\teval-rmspe:0.127860\ttrain-rmspe:0.173695\n",
      "[151]\teval-rmspe:0.127549\ttrain-rmspe:0.173274\n",
      "[152]\teval-rmspe:0.127373\ttrain-rmspe:0.173336\n",
      "[153]\teval-rmspe:0.127223\ttrain-rmspe:0.173222\n",
      "[154]\teval-rmspe:0.127065\ttrain-rmspe:0.173089\n",
      "[155]\teval-rmspe:0.126920\ttrain-rmspe:0.163185\n",
      "[156]\teval-rmspe:0.126806\ttrain-rmspe:0.163060\n",
      "[157]\teval-rmspe:0.126813\ttrain-rmspe:0.163019\n",
      "[158]\teval-rmspe:0.126639\ttrain-rmspe:0.162797\n",
      "[159]\teval-rmspe:0.126497\ttrain-rmspe:0.162600\n",
      "[160]\teval-rmspe:0.125735\ttrain-rmspe:0.162432\n",
      "[161]\teval-rmspe:0.125594\ttrain-rmspe:0.162315\n",
      "[162]\teval-rmspe:0.125450\ttrain-rmspe:0.162285\n",
      "[163]\teval-rmspe:0.125229\ttrain-rmspe:0.161958\n",
      "[164]\teval-rmspe:0.125129\ttrain-rmspe:0.162231\n",
      "[165]\teval-rmspe:0.125074\ttrain-rmspe:0.162009\n",
      "[166]\teval-rmspe:0.124923\ttrain-rmspe:0.161976\n",
      "[167]\teval-rmspe:0.124890\ttrain-rmspe:0.162111\n",
      "[168]\teval-rmspe:0.124744\ttrain-rmspe:0.162004\n",
      "[169]\teval-rmspe:0.124589\ttrain-rmspe:0.161945\n",
      "[170]\teval-rmspe:0.124324\ttrain-rmspe:0.160044\n",
      "[171]\teval-rmspe:0.124320\ttrain-rmspe:0.159985\n",
      "[172]\teval-rmspe:0.123971\ttrain-rmspe:0.159474\n",
      "[173]\teval-rmspe:0.123867\ttrain-rmspe:0.159371\n",
      "[174]\teval-rmspe:0.123793\ttrain-rmspe:0.159251\n",
      "[175]\teval-rmspe:0.123677\ttrain-rmspe:0.159188\n",
      "[176]\teval-rmspe:0.123599\ttrain-rmspe:0.159130\n",
      "[177]\teval-rmspe:0.123463\ttrain-rmspe:0.158938\n",
      "[178]\teval-rmspe:0.123356\ttrain-rmspe:0.150136\n",
      "[179]\teval-rmspe:0.123275\ttrain-rmspe:0.150014\n",
      "[180]\teval-rmspe:0.122969\ttrain-rmspe:0.149745\n",
      "[181]\teval-rmspe:0.122964\ttrain-rmspe:0.149703\n",
      "[182]\teval-rmspe:0.122747\ttrain-rmspe:0.145405\n",
      "[183]\teval-rmspe:0.122677\ttrain-rmspe:0.145364\n",
      "[184]\teval-rmspe:0.122628\ttrain-rmspe:0.145204\n",
      "[185]\teval-rmspe:0.122083\ttrain-rmspe:0.144920\n",
      "[186]\teval-rmspe:0.122008\ttrain-rmspe:0.144857\n",
      "[187]\teval-rmspe:0.121845\ttrain-rmspe:0.144660\n",
      "[188]\teval-rmspe:0.121803\ttrain-rmspe:0.144578\n",
      "[189]\teval-rmspe:0.121663\ttrain-rmspe:0.144384\n",
      "[190]\teval-rmspe:0.121549\ttrain-rmspe:0.144226\n",
      "[191]\teval-rmspe:0.121540\ttrain-rmspe:0.144165\n",
      "[192]\teval-rmspe:0.121367\ttrain-rmspe:0.144000\n",
      "[193]\teval-rmspe:0.120854\ttrain-rmspe:0.143886\n",
      "[194]\teval-rmspe:0.120682\ttrain-rmspe:0.143703\n",
      "[195]\teval-rmspe:0.120631\ttrain-rmspe:0.143551\n",
      "[196]\teval-rmspe:0.120495\ttrain-rmspe:0.143442\n",
      "[197]\teval-rmspe:0.120424\ttrain-rmspe:0.143383\n",
      "[198]\teval-rmspe:0.120205\ttrain-rmspe:0.143255\n",
      "[199]\teval-rmspe:0.120000\ttrain-rmspe:0.142845\n",
      "[200]\teval-rmspe:0.119894\ttrain-rmspe:0.142756\n",
      "[201]\teval-rmspe:0.119827\ttrain-rmspe:0.142712\n",
      "[202]\teval-rmspe:0.119719\ttrain-rmspe:0.142615\n",
      "[203]\teval-rmspe:0.119558\ttrain-rmspe:0.142994\n",
      "[204]\teval-rmspe:0.119600\ttrain-rmspe:0.142791\n",
      "[205]\teval-rmspe:0.119527\ttrain-rmspe:0.142688\n",
      "[206]\teval-rmspe:0.119451\ttrain-rmspe:0.130166\n",
      "[207]\teval-rmspe:0.119350\ttrain-rmspe:0.130149\n",
      "[208]\teval-rmspe:0.119287\ttrain-rmspe:0.127460\n",
      "[209]\teval-rmspe:0.119124\ttrain-rmspe:0.127351\n",
      "[210]\teval-rmspe:0.118919\ttrain-rmspe:0.124146\n",
      "[211]\teval-rmspe:0.118773\ttrain-rmspe:0.123991\n",
      "[212]\teval-rmspe:0.118653\ttrain-rmspe:0.123903\n",
      "[213]\teval-rmspe:0.118479\ttrain-rmspe:0.123735\n",
      "[214]\teval-rmspe:0.118043\ttrain-rmspe:0.123526\n",
      "[215]\teval-rmspe:0.117671\ttrain-rmspe:0.123330\n",
      "[216]\teval-rmspe:0.117650\ttrain-rmspe:0.123320\n",
      "[217]\teval-rmspe:0.117627\ttrain-rmspe:0.123255\n",
      "[218]\teval-rmspe:0.117487\ttrain-rmspe:0.123203\n",
      "[219]\teval-rmspe:0.117420\ttrain-rmspe:0.123126\n",
      "[220]\teval-rmspe:0.117317\ttrain-rmspe:0.122404\n",
      "[221]\teval-rmspe:0.117220\ttrain-rmspe:0.122270\n",
      "[222]\teval-rmspe:0.117162\ttrain-rmspe:0.122200\n",
      "[223]\teval-rmspe:0.117175\ttrain-rmspe:0.122143\n",
      "[224]\teval-rmspe:0.117084\ttrain-rmspe:0.122047\n",
      "[225]\teval-rmspe:0.117047\ttrain-rmspe:0.121985\n",
      "[226]\teval-rmspe:0.117011\ttrain-rmspe:0.121927\n",
      "[227]\teval-rmspe:0.116977\ttrain-rmspe:0.121857\n",
      "[228]\teval-rmspe:0.116962\ttrain-rmspe:0.122470\n",
      "[229]\teval-rmspe:0.116838\ttrain-rmspe:0.122422\n",
      "[230]\teval-rmspe:0.116749\ttrain-rmspe:0.122331\n",
      "[231]\teval-rmspe:0.116599\ttrain-rmspe:0.122212\n",
      "[232]\teval-rmspe:0.116469\ttrain-rmspe:0.122081\n",
      "[233]\teval-rmspe:0.116365\ttrain-rmspe:0.121993\n",
      "[234]\teval-rmspe:0.116314\ttrain-rmspe:0.121882\n",
      "[235]\teval-rmspe:0.116238\ttrain-rmspe:0.121806\n",
      "[236]\teval-rmspe:0.116199\ttrain-rmspe:0.121683\n",
      "[237]\teval-rmspe:0.116113\ttrain-rmspe:0.121603\n",
      "[238]\teval-rmspe:0.116052\ttrain-rmspe:0.121523\n",
      "[239]\teval-rmspe:0.115921\ttrain-rmspe:0.121404\n",
      "[240]\teval-rmspe:0.115871\ttrain-rmspe:0.121314\n",
      "[241]\teval-rmspe:0.115833\ttrain-rmspe:0.121212\n",
      "[242]\teval-rmspe:0.115743\ttrain-rmspe:0.121087\n",
      "[243]\teval-rmspe:0.115691\ttrain-rmspe:0.118505\n",
      "[244]\teval-rmspe:0.115693\ttrain-rmspe:0.118438\n",
      "[245]\teval-rmspe:0.115584\ttrain-rmspe:0.118205\n",
      "[246]\teval-rmspe:0.115686\ttrain-rmspe:0.118115\n",
      "[247]\teval-rmspe:0.115618\ttrain-rmspe:0.118026\n",
      "[248]\teval-rmspe:0.115536\ttrain-rmspe:0.117869\n",
      "[249]\teval-rmspe:0.115446\ttrain-rmspe:0.117765\n",
      "[250]\teval-rmspe:0.115381\ttrain-rmspe:0.117714\n",
      "[251]\teval-rmspe:0.115286\ttrain-rmspe:0.117594\n",
      "[252]\teval-rmspe:0.115236\ttrain-rmspe:0.117735\n",
      "[253]\teval-rmspe:0.115008\ttrain-rmspe:0.117580\n",
      "[254]\teval-rmspe:0.115038\ttrain-rmspe:0.117496\n",
      "[255]\teval-rmspe:0.114881\ttrain-rmspe:0.117366\n",
      "[256]\teval-rmspe:0.114771\ttrain-rmspe:0.117217\n",
      "[257]\teval-rmspe:0.114785\ttrain-rmspe:0.117153\n",
      "[258]\teval-rmspe:0.114779\ttrain-rmspe:0.117104\n",
      "[259]\teval-rmspe:0.114720\ttrain-rmspe:0.117070\n",
      "[260]\teval-rmspe:0.114706\ttrain-rmspe:0.117027\n",
      "[261]\teval-rmspe:0.114638\ttrain-rmspe:0.116932\n",
      "[262]\teval-rmspe:0.114664\ttrain-rmspe:0.116871\n",
      "[263]\teval-rmspe:0.114622\ttrain-rmspe:0.116762\n",
      "[264]\teval-rmspe:0.114573\ttrain-rmspe:0.116700\n",
      "[265]\teval-rmspe:0.114616\ttrain-rmspe:0.116603\n",
      "[266]\teval-rmspe:0.114562\ttrain-rmspe:0.116568\n",
      "[267]\teval-rmspe:0.114528\ttrain-rmspe:0.116479\n",
      "[268]\teval-rmspe:0.114514\ttrain-rmspe:0.116334\n",
      "[269]\teval-rmspe:0.114456\ttrain-rmspe:0.116256\n",
      "[270]\teval-rmspe:0.114434\ttrain-rmspe:0.116218\n",
      "[271]\teval-rmspe:0.114351\ttrain-rmspe:0.116142\n",
      "[272]\teval-rmspe:0.114306\ttrain-rmspe:0.115994\n",
      "[273]\teval-rmspe:0.114236\ttrain-rmspe:0.115929\n",
      "[274]\teval-rmspe:0.114127\ttrain-rmspe:0.115812\n",
      "[275]\teval-rmspe:0.114124\ttrain-rmspe:0.115726\n",
      "[276]\teval-rmspe:0.114054\ttrain-rmspe:0.115643\n",
      "[277]\teval-rmspe:0.113987\ttrain-rmspe:0.115548\n",
      "[278]\teval-rmspe:0.113967\ttrain-rmspe:0.115422\n",
      "[279]\teval-rmspe:0.113902\ttrain-rmspe:0.115341\n",
      "[280]\teval-rmspe:0.113862\ttrain-rmspe:0.115273\n",
      "[281]\teval-rmspe:0.113817\ttrain-rmspe:0.115235\n",
      "[282]\teval-rmspe:0.113720\ttrain-rmspe:0.115147\n",
      "[283]\teval-rmspe:0.113622\ttrain-rmspe:0.115068\n",
      "[284]\teval-rmspe:0.113538\ttrain-rmspe:0.115016\n",
      "[285]\teval-rmspe:0.113525\ttrain-rmspe:0.114976\n",
      "[286]\teval-rmspe:0.113437\ttrain-rmspe:0.114892\n",
      "[287]\teval-rmspe:0.113389\ttrain-rmspe:0.114821\n",
      "[288]\teval-rmspe:0.113355\ttrain-rmspe:0.114775\n",
      "[289]\teval-rmspe:0.113323\ttrain-rmspe:0.114716\n",
      "[290]\teval-rmspe:0.113261\ttrain-rmspe:0.114637\n",
      "[291]\teval-rmspe:0.113033\ttrain-rmspe:0.114469\n",
      "[292]\teval-rmspe:0.112958\ttrain-rmspe:0.114412\n",
      "[293]\teval-rmspe:0.112941\ttrain-rmspe:0.114383\n",
      "[294]\teval-rmspe:0.112918\ttrain-rmspe:0.114318\n",
      "[295]\teval-rmspe:0.112953\ttrain-rmspe:0.114264\n",
      "[296]\teval-rmspe:0.112766\ttrain-rmspe:0.113917\n",
      "[297]\teval-rmspe:0.112692\ttrain-rmspe:0.113831\n",
      "[298]\teval-rmspe:0.112563\ttrain-rmspe:0.113699\n",
      "[299]\teval-rmspe:0.112521\ttrain-rmspe:0.113594\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = cross_validation.train_test_split(train, test_size=0.0113)\n",
    "dtrain = xgb.DMatrix(X_train[features].as_matrix(), np.log(X_train[\"Sales\"] + 1))\n",
    "dvalid = xgb.DMatrix(X_test[features].as_matrix(), np.log(X_test[\"Sales\"] + 1))\n",
    "dtest = xgb.DMatrix(test[features].as_matrix())\n",
    "watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "gbm = xgb.train(params, dtrain, num_trees, evals=watchlist, early_stopping_rounds=50, feval=rmspe_xg, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('error', 0.11252069625715708)\n"
     ]
    }
   ],
   "source": [
    "# Validate predictions\n",
    "train_probs = gbm.predict(xgb.DMatrix(X_test[features].as_matrix()))\n",
    "indices = train_probs < 0\n",
    "train_probs[indices] = 0\n",
    "error = rmspe(np.exp(train_probs) - 1, X_test['Sales'].values)\n",
    "print('error', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make predictions on the test set\n",
    "test_probs = gbm.predict(xgb.DMatrix(test[features].as_matrix()))\n",
    "indices = test_probs < 0\n",
    "test_probs[indices] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"Id\": test[\"Id\"], \"Sales\": np.exp(test_probs) - 1})\n",
    "submission.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
