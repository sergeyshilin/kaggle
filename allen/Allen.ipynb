{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import wikipedia as wiki\n",
    "from urllib2 import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(review, remove_stopwords = True ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    # 1. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    # 2. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # 3. Optionally remove stop words (true by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    # 5. Return a list of words\n",
    "    return words\n",
    "\n",
    "def ensure_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keyword_from_url_topic(url_topic):\n",
    "    # Topic includes: Earth Science, Life Science, Physical Science, Biology, Chemestry and Physics\n",
    "    lst_url = []\n",
    "    html = urlopen(url_topic).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for tag_h3 in soup.find_all('h3'):\n",
    "        url_res =  ' '.join(tag_h3.li.a.get('href').strip('/').split('/')[-1].split('-'))\n",
    "        lst_url.append(url_res)\n",
    "    return lst_url\n",
    "\n",
    "\n",
    "def get_save_wiki_docs(keywords, save_folder = 'data/wiki_data/'):\n",
    "    \n",
    "    ensure_dir(save_folder)\n",
    "    \n",
    "    n_total = len(keywords)\n",
    "    for i, kw in enumerate(keywords):\n",
    "        kw = kw.lower()\n",
    "        print i, n_total, i * 1.0 / n_total, kw\n",
    "        try:\n",
    "            content = wiki.page(kw).content.encode('ascii', 'ignore')\n",
    "        except wiki.exceptions.DisambiguationError as e:\n",
    "            print 'DisambiguationError', kw\n",
    "        except:\n",
    "            print 'Error', kw\n",
    "        if not content:\n",
    "            continue\n",
    "        with open(os.path.join(save_folder, '_'.join(kw.split()) + '.txt'), 'w') as f:\n",
    "                f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docstf_idf(dir_data):\n",
    "    \"\"\" indexing wiki pages:\n",
    "    returns {document1:{word1:tf, word2:tf ...}, ....},\n",
    "            {word1: idf, word2:idf, ...}\"\"\"\n",
    "    docs_tf = {}\n",
    "    idf = {}\n",
    "    vocab = set()\n",
    "\n",
    "    for fname in os.listdir(dir_data):\n",
    "        dd = {}\n",
    "        total_w = 0\n",
    "        path = os.path.join(dir_data, fname)\n",
    "        for index, line in enumerate(open(path)):\n",
    "            lst = tokenize(line)\n",
    "            for word in lst:\n",
    "                vocab.add(word)\n",
    "                dd.setdefault(word, 0)\n",
    "                dd[word] += 1\n",
    "                total_w += 1 \n",
    "        \n",
    "        for k, v in dd.iteritems(): \n",
    "            dd[k] = 1.* v / total_w\n",
    "        \n",
    "        docs_tf[fname] = dd\n",
    "    \n",
    "    for w in list(vocab):\n",
    "        docs_with_w = 0\n",
    "        for path, doc_tf in docs_tf.iteritems():\n",
    "            if w in doc_tf:\n",
    "                docs_with_w += 1\n",
    "        idf[w] = log(len(docs_tf)/docs_with_w)\n",
    "\n",
    "    return docs_tf, idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docs_importance_for_question(question, dosc_tf, word_idf, max_docs = None):\n",
    "    question_words = set(tokenize(question))\n",
    "    #go through each article\n",
    "    doc_importance = []\n",
    "\n",
    "    for doc, doc_tf in dosc_tf.iteritems():\n",
    "        doc_imp = 0\n",
    "        for w in question_words:\n",
    "            if w in doc_tf:\n",
    "                doc_imp += doc_tf[w]  * word_idf[w]\n",
    "        doc_importance.append((doc, doc_imp))\n",
    "    \n",
    "    #sort doc importance    \n",
    "    doc_importance = sorted(doc_importance, key=lambda x: x[1], reverse = True)\n",
    "    if max_docs:\n",
    "        return doc_importance[:max_docs]\n",
    "    else:\n",
    "        return doc_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#urls  to get toppics\n",
    "ck12_url_topic = ['https://www.ck12.org/earth-science/', 'http://www.ck12.org/life-science/', \n",
    "                  'http://www.ck12.org/physical-science/', 'http://www.ck12.org/biology/', \n",
    "                  'http://www.ck12.org/chemistry/', 'http://www.ck12.org/physics/']\n",
    "wiki_docs_dir = 'data/wiki_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wiki_docs():\n",
    "    # get keywords \n",
    "    ck12_keywords = set()\n",
    "    for url_topic in ck12_url_topic:\n",
    "        keywords= get_keyword_from_url_topic(url_topic)\n",
    "        for kw in keywords:\n",
    "            ck12_keywords.add(kw)\n",
    "    \n",
    "    #get and save wiki docs\n",
    "    get_save_wiki_docs(ck12_keywords, wiki_docs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
